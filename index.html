---
layout: single
author_profile: true
---
<script type="text/javascript" src="assets/js/hidebib.js"></script>
<style>
    a {
        text-decoration: none;
    }

    li{
    margin: 1px 0;
    }

    heading {
    
    font-size: 16px;
    font-weight: bold;
    margin: 0px;
    /* color: #11999e; */
    }
    head_under{
    font-size: 14px; 
    margin: 0px; 
    padding:0px;
    display: block;
    }
    toggle_down{
        font-size:14px; 
        text-align: justify;
        font-style:italic;
        display: block;
        
    }
      
</style>
<!-- <br> -->
<p style="font-size: 20px" ><b><center>Site in Progress</center></b></p>
<div id="about_me">
<p style="font-size:16px;">
I am a Master's student at the <a href='https://www.ri.cmu.edu/' target="_blank">Robotics Institute</a>, 
School of Computer Science, Carnegie Mellon University, 
working with <a href='https://en.wikipedia.org/wiki/Howie_Choset' target="_blank">Prof. Howie Choset</a> and <a href='https://www.ri.cmu.edu/ri-faculty/matthew-j-travers/' target="_blank">Prof. Matthew Travers</a>. 
I am  working on deep-learning to optimize autonomous exploration and navigation with limited perception in unknown dynamic/static environments.
<br>
<br>
My interests are in the research and development of intelligent systems and integrated robotics technologies to optimize and improve human life. 
Broadly in  machine learning, computer vision, planning and systems engineering particularly in perception, deep-learning and decision-making.
</p>
</div>

<div id="highlights"> 
<h2> Highlights </h2>
<ul style="font-size:16px">
    <li >May 2019 &nbsp Started <a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-robotics/" target="_blank">Master of Science in Robotics</a>.</li>
    <li>May 2019 &nbsp Graduated accelerated 3-year B.S. in <a href="https://www.ece.cmu.edu/" target="_blank">ECE</a>.</li>
    <li>May 2019 &nbsp Co-led <a href="https://sites.google.com/view/cmu-jollyroger/home" target="_blank">JollyRoger</a> team to win 1st place award 
        of $5,500 by <a href="https://www.leidos.com/" target="_blank">Leidos</a> in Mechatronic Design Capstone project: ShipBot, CMU.</li>
    <li>Dec 2018 &nbsp Dean’s List CMU.</li>
    <li>May-Aug 2018 &nbsp Worked at the intersection of perception and action in the Advanced Robotics Perception Team,<a href="https://www.amazonrobotics.com/#/" target="_blank"> Amazon Robotics</a>.</li>
    <li>Apr 2018 &nbsp Won the <a href="https://www.cmu.edu/uro/MoM/2018-award-winners.html" target="_blank">Boeing Blue Skies Award: Game Changer</a> $1,000 for one of two presented research work 
        in <a href="https://www.cmu.edu/uro/MoM/" target="_blank">Meeting of the Minds</a> Research Symposium.</li>       
    <li>Jun 2017 &nbsp Awarded <a href='https://www.cmu.edu/uro/summer%20research%20fellowships/SURF/' target="_blank">Summer Undergraduate Research Fellowship</a> $3500 CMU.</li>       
    <li>May 2017 &nbsp Appointed as Motion Software Lead on CMU Research Team containing research staff in collaboration with Boeing.</li>
    <li>Sep 2016 &nbsp Started working in the <a href="http://biorobotics.ri.cmu.edu/" target="_blank">BioRobotics Lab</a>.</li>
    <li>Jun 2015 &nbsp Awarded <a href="http://online-inspire.gov.in/Account/INSPIREProgramme" target="_blank">INSPIRE</a> Internship, selected from top 1% rankers in their X Board Examinations 
        by <a href="dst.gov.in" target="_blank">DST, Gov. of India</a>.</li>
    <li>Aug 2014 &nbsp Invited to be <a href="http://my.rsc.org/home" target="_blank">Chemnet</a> writer at <a href="https://www.rsc.org/" target="_blank">Royal Society of Chemistry UK</a>.</li>
    <li>May 2014 &nbsp <a href="http://www.arkwright.org.uk/" target="_blank">Arkwright Engineering Scholarship</a> finalist.</li>
    <li>2013 &nbsp Certificate of Merit Senior Kangaroo United Kingdom Mathematics Trust, UK.</li>
    <li>2013 &nbsp Gold in Senior Maths Challenge United Kingdom Mathematics Trust, UK.</li>
    <li>2013 &nbsp Gold in Intermediate Maths Challenge United Kingdom Mathematics Trust, UK.</li>
   
</ul>
</div>

<div id="experience">
<h2>Experience</h2> 

<heading>Boeing Blaser Project | Carnegie Mellon University & Boeing Collaboration </heading>
<head_under>Graduate Research Assistant | Howie Choset <span style="float:right;">May 2019 – Present</span></head_under>
<a href="javascript:toggleblock('bb_abs')" style="font-size:14px">abstract</a> | 
<a href="http://biorobotics.ri.cmu.edu/research/ConfinedSpacePerception.php" style="font-size:14px" target="_blank">webpage</a> | 
<a href=/ style="font-size:14px" target="_blank">slides</a> 
<toggle_down id="bb_abs">
<br>
Designing VIO algorithms for running onboard an i.MX RT platform for developing a standalone miniature size, ultra-short range, and high accuracy laser-camera sensor module for applications in confined space. <br>
Developed a unified calibration procedure for simultaneous non-linear optimization on 9 parameters of camera intrinsics, laser-to-camera and hand-eye calibration.<br>
Redesigned algorithm to speed up procedure run-time by 5x, over &lt30 images for 640x480 and 1280x960 camera resolutions. 
<br>
<b>Software</b>: C/C++, Python, Matlab, ROS<br>
<b>Hardware</b>: Blaser Sensor, UR5e<br>
<b>Key Concepts</b>: camera calibration, hand-eye calibration, camera-laser calibration, visual-inertial odometry, short-range high-precision perception
</toggle_down><br> <br>

<heading>Boeing Sealant Application | Carnegie Mellon University & Boeing Collaboration </heading>
<head_under>Motion Software Lead | Howie Choset <span style="float:right;">May 2017 – May 2019</span></head_under>
<a href="javascript:toggleblock('bs_abs')" style="font-size:14px" >abstract</a> | 
<a href="http://biorobotics.ri.cmu.edu/research/ConfinedSpaceManipulator.php" style="font-size:14px" target="_blank">webpage</a> | 
<a href=/ style="font-size:14px" target="_blank">slides</a> 
<toggle_down id="bs_abs">
<br>
Developed and implemented the closed-loop, high-precision motion control infrastructure and software with computer vision feedback for Boeing APADA Arm, automating sealant application and inspection on sub-millimeter cracks in aircraft wing bays. This solution reduces human error and improves worker health-safety as the sealant is noxious.<br>
Developed multiple trajectories for robot arm, based on permutations and combinations in shapes of cones and spirals to maximize scanning coverage area of the mounted sensor and nozzle of robot arm for sealant application. <br>
Implemented tracking and following of finger using short-range high-precision custom laser-camera sensor mounted on UR5e to demonstrate the high-precision closed loop control of the sensor and robot arm for confined space perception and manipulation. 
<br>
<b>Software</b>: C/C++, ROS<br>
<b>Hardware</b>: Custom 5DOF Arm, UR5e, Blaser sensor<br>
<b>Key Concepts</b>: kinematics, trajectory generation, motion planning, obstacle navigation, short-range computer vision
</toggle_down><br> <br>

<heading>Amazon Robotics | Advanced Robotics Perception Team </heading>
<head_under>Advanced Robotics Intern | Chuck Llyod <span style="float:right;">May – Aug 2018</span></head_under>
<!-- Text with Toggle -->
<a href="javascript:toggleblock('am_abs')" style="font-size:14px">abstract</a> |
<a href=/ style="font-size:14px" target="_blank" >media</a> 

<!-- Content to show after toggle -->
<toggle_down id="am_abs">
<br>
Developed a computer vision based solution to the multifaceted issue of failed picks in the pick and place pipeline to stow away the items that come into the Amazon warehouse. I investigated and analyzed the existing pipeline to pinpoint the problem, incorrectly perceived 3D geometry caused by “holes” of missing data. This occured due to occlusion, reflectance, and the scanning angle of the static multi-sensor workspace rig.
Thus my solution pipeline addressed this problem by: <br>
• Identifying and locating the holes through angle criterion<br>
• Validation of missing data versus physical hole<br>
• Prioritizing filling order and executing re-planned picking path for scanning<br>
The solution implemented is a hybrid pipeline using a sensor mounted on the end-effector of the robot to fill in data after detecting and confirming the existence of an area with missing data from image and point cloud space.<br>  
Implemented the static sensors camera rig and environmental constraints of as-is implementation on the workstation; designed and 3D printed end-of-arm sensor mount. Demonstrated integrated modular pipeline on physical system.<br>
Also developed an alternate solution using classical techniques through PCL of surface reconstruction including triangulation, poisson reconstruction, voxel dilation etc; Pitched alternate solution for fitting shape primitives to approximate shape and used machine learning models to predict the geometry of the object.<br>

<b>Software</b>: C/C++, ROS<br>
<b>Hardware</b>: IntelRealsense D410, Ensenso, UR10<br>
<b>Key Concepts</b>: sensor-fusion, motion planning, computer vision, point cloud to image reprojection
</toggle_down><br> <br>



<heading>FOXCONN IMR Fast-Vision | Carnegie Mellon University & FOXCONN</heading> 
<head_under>Research Assistant <span style="float:right;">May – Aug 2017</span></head_under>
<a href="javascript:toggleblock('fs_abs')" style="font-size:14px" >abstract</a> |
<a href="assets/files/slides/" style="font-size:14px" target="_blank">slides</a> |
<a href="assets/images/" style="font-size:14px" target="_blank">media</a>

<toggle_down id="fs_abs">
<br>
Developed in-lab low-cost optical-flow sensor, containing two high fps cameras and an IMU, for high-speed localization of FOXCONN modular robots for multi-purpose use in dense industrial environment with serial communication 
and dynamic parameterization with ROS. Analyzed and selected apt off-the-shelf OEM barcode scanners, to develop this sensor
<br>
<b>Software</b>: Arduino, C/C++, ROS<br>
<b>Hardware</b>: IMU, Monocular Cameras<br>
<b>Key Concepts</b>: optical-flow, sensor-fusion, localization

</toggle_down><br> <br>

<heading>Summer Undergraduate Research Fellowship</heading>
<head_under>Research Assistant | Howie Choset  <span style="float:right;">May – Aug 2017</span></head_under>
<a href="javascript:toggleblock('surf_abs')" style="font-size:14px">abstract</a> | 
<a href="assets/images/mom_media.jpg" style="font-size: 14px" target="_blank">media</a>
<toggle_down id="surf_abs">
<br>
Awarded the Summer Undergraduate Research Fellowship 2017 for “Stable Stair Climbing for a Hexapod using Onboard Sensing” under Prof. Howie Choset and Dr. Guillaume. 
Researched and developed a gait, using Central Pattern Generators, for a hexapod robot in general terrain that is parameterizable by dimensions of obstacle extracted from camera feedback to make an ‘intuitive’ seamless 'walk to climb' transition increasing mobility, particularly for the introduction of legged companion robots in homes. <br>
Awarded the Boeing Blue Skies Award: GameChanger at the Meeting of the Minds Symposium 2018.
<br>
<b>Software</b>: Python, ROS<br>
<b>Hardware</b>: Hexapod, Kinect <br>
<b>Key Concepts</b>: cpgs, computer vision, dynamic motion planning
</toggle_down><br> <br>

<heading>Gearless Omni-directional Acceleration-vectoring Topology Project</heading>
<head_under>Undergraduate Researcher | Matt Travers <span style="float:right;">Dec 2016 – Jun 2017</span></head_under>
<a href="javascript:toggleblock('goat_abs')" style="font-size:14px">abstract</a>
<toggle_down id="goat_abs">
<br>
Developed ROS teleoperation of a robotic omni-directional leg to control motion and jump over obstacles.
Built model and simulation for testing scenarios in physics simulator Gazebo
<br>
<b>Software</b>: Python, XML, ROS, Gazebo<br>
<b>Hardware</b>: A Quasi-Direct Drive Legged Robot <br>
<b>Key Concepts</b>: simulation, teleoperation   
</toggle_down><br> <br>

<heading>Series Elastic Actuator Robots</heading>
<head_under>Undergraduate Researcher | BioRobotics Lab<span style="float:right;">Sep 2016 – Feb 2017</span></head_under>
<a href="javascript:toggleblock('sea_abs')" style="font-size:14px">abstract</a>
<toggle_down id="sea_abs">
<br>
Investigated methods for freeing the Snake Robot from a trapped position in uneven terrain and move towards initial heading using signal analysis on in-built torque sensor readings. <br>
Verified the heuristic we proposed to detect the jammed state by detecting repetitive signals from the sensor feedback in the robot links. The low-cost solution implemented as an interim solution was to reduce speed, as the robot was very unlikely to get trapped at lower speeds.<br>
Worked on optimizing LED driver for housing more components or scaling down Snake robot for constrained environments.
<br>
<b>Software</b>: Python, Matlab, CircuitMaker<br>
<b>Hardware</b>: Snake robot <br>
<b>Key Concepts</b>: signal-analysis, fourier transforms, circuits
</toggle_down><br> <br>

</div>

<div id="project">
<h2>Projects</h2> 

<heading>18-578 Mechatronic Capstone Design: ShipBot </heading>
<head_under>Team: David Bang, <b>Sara Misra</b>, Fiona Li, Haowen Shi, Bo Tian <span style="float:right;">Spring 2019</span></head_under>
<a href="javascript:toggleblock('578_abs')" style="font-size:14px">abstract</a> |
<a href="https://sites.google.com/view/cmu-jollyroger/home" style="font-size:14px" target="_blank">webpage</a> | 
<a href="assets/files/TeamA_FinalReport.pdf" style="font-size: 14px" target="_blank">report</a> | 
<a href="https://github.com/cmu-jollyroger" style="font-size:14px" target="_blank">code</a>
<toggle_down id="578_abs">
<br>
For this project we built an autonomous robot that could autonomously manipulate a fixed set of 
electro-mechanical devices such as different types of valves and breaker switches given some high-level commands.
This project provides a proof-of-concept mechatronic device capable of operating 
without modifying the existing human-operated devices reducing the cost of retrofitting autonomous navigation onto 
long-distance ships. <br>
I designed, developed and implemented the localization algorithm, identification of the electromechanical device to manipulate, mounted robotic manipulator arm kinematics and vision-based closed-loop motion planning. 
Responsible for sensor placement and design of hardware chassis frame, as well as contributed to the cyber-physical and software system.
<br>
<b>Software</b>: C/C++, XML, ROS<br>
<b>Hardware</b>: We built the robot! <br>
<b>Key Concepts</b>: autonomy, computer vision, arm kinematics, motion-planning, localization
 </toggle_down>

<br> <br>

<heading>10-701 Semi-Supervised Learning for Classification </heading>
<head_under>Team: Wenyu Huang, <b>Sara Misra</b>, Junyan Pu <span style="float:right;">Spring 2019</span></head_under>
<a href="javascript:toggleblock('701_abs')" style="font-size:14px">abstract</a> | 
<a href="assets/files/10_701_final_report.pdf" style="font-size: 14px" target="_blank">report</a> | 
<a href="assets/files/10701 Poster Presentation_ Group 34.pdf" style="font-size: 14px" target="_blank">poster</a> 
<toggle_down id="701_abs">
<br>
Challenge : <a href="https://www.kaggle.com/c/forest-cover-type-prediction" target="_blank">Forest Type Classification</a>
<br>We predict the forest cover type of an area given cartographical features of the area using a semi-supervised approach to deal with the lack of labeled data. We explore multiple methods of semi-supervised learning, particularly the Naive Bayes and Expectation Maximization algorithm, S3VM, and co-training using neural networks.
<br>
<b>Software</b>: Python<br>
<b>Key Concepts</b>: expectation-maximization, SV3M, co-training, neural-networks

</toggle_down> <br> <br>

<heading>16-720 Augmented Reality </heading>
<head_under>Team: Naman Gupta, <b>Sara Misra</b>, Olivia Xu <span style="float:right;">Spring 2019</span></head_under>
<a href="javascript:toggleblock('720_abs')" style="font-size:14px">abstract</a> | 
<a href="assets/files/slides/16720 Augmented Reality.pptx" style="font-size: 14px" target="_blank">slides</a> | 
<a href="https://github.com/saram1sra/16720_Augmented_Reality" style="font-size: 14px" target="_blank">code</a> 
<toggle_down id="720_abs">
<br>
The project goal was to overlay a 3D wavefront obj model onto a video recorded using an iphone camera. This project is influenced by pokemon Go and how we can bring augmented reality to physical Pokemon playing cards. 
<br>
<b>Software</b>: Python<br>
<b>Hardware</b>: iPhone, Pokemon playing cards <br>
<b>Key Concepts</b>: classification, homography estimation, 3Dto2D projection, ORB descriptors
</toggle_down> <br> <br>

<heading>15-112 Interactive Interview Bot </heading>
<head_under> <span style="float:right;">Nov – Dec 2016    </span></head_under>
<a href="javascript:toggleblock('112_abs')" style="font-size:14px">abstract</a> | 
<a href="assets/files/15112 Final Report.pdf" style="font-size: 14px" target="_blank">report</a> |
<a href="https://www.youtube.com/watch?v=U7lLb5Ke5hE" style="font-size: 14px" target="_blank">video</a> |
<a href="https://github.com/saram1sra/15112_InterviewBot" style="font-size: 14px" target="_blank">code</a>
<toggle_down id="112_abs">
<br>
Developed an interactive Interview Bot to minimize human error and bias in interviewing candidates to  assess candidate personality and soft-skill fit for the applied position, replacing human interviewers for first-stage interviews. 
<br>Utilizing IBM Watson’s AI, the system’s core was an Input-Process-Output-Feedback (Learn) control loop, bringing human-like cognition to machines which could be further developed by using machine learning algorithms.
<br>Features: <br>
• Voice-based interaction with the candidate  <br>
• Analyzes the candidate responses to assess their 47 personality traits<br>
• Supports 761 job positions from Researcher to DJ to assess candidates on<br>
• Recommends jobs with best personality fit to the candidate       <br>                             	             
<b>Software</b>: Python <br>
<b>Frameworks</b>: IBM Watson, Google Text-to-Speech<br>
<b>Key Concepts</b>: discretize human personality, human-computer interaction

</toggle_down> <br> <br>

<heading>PennApps Hack XIV: Take Me Out!</heading>
<head_under>Team: Daniel Barychev, Guangyu Chen, <b>Sara Misra</b>, Alex Rudenko  <span style="float:right;"> Fall 2016   </span></head_under>
<a href="javascript:toggleblock('tmo_abs')" style="font-size:14px">abstract</a> | 
<a href="https://devpost.com/software/take-me-out-pirfb5" style="font-size:14px" target="_blank">webpage</a> 
<br>
<toggle_down id="tmo_abs">
<br>
Take Me Out presents the user with an option for a night out, simply a button click away.
From user response through like and dislike buttons, the app collects data 
from the user and uses it to make more informed decisions from recommendation to recommendation. 
<br> 
I designed and developed the app design and user interface.
</toggle_down><br> <br>
</div>


<div id="disc">
<h2>Publication</h2>

<heading><a href="https://priorart.ip.com/IPCOM/000244611D">SmartHealth@PlantIntelligence</a></heading>
<head_under> <span style="float:right;">29 Dec 2015</span></head_under>
<a href="javascript:toggleblock('disc_abs')" style="font-size: 14px">abstract</a> 
<toggle_down id="disc_abs">
<br>
IP.COM DISCLOSURE NUMBER: IPCOM000244611D <br>
I conceptualized and co-authored a cognitive based solution to improve human 
health by recommending right kind and number of plants based on different influencing parameters 
e.g. individual/combined families’ demographic, bio-metric, emotional, financial, life-events, 
social and others including environmental factors/parameters. 
I published this work as a disclosure through IBM; the ‘intelligence’ of the conceptualized solution 
could be further enhanced using deep learning algorithms.
</toggle_down> <br> <br>
</div>





<!-- Init hideblock for toggle -->
<script xml:space="preserve" language="JavaScript">
        hideblock('bb_abs');
</script>
<script xml:space="preserve" language="JavaScript">
        hideblock('bs_abs');
</script>
<script xml:space="preserve" language="JavaScript">
    hideblock('am_abs');
 </script>
<script xml:space="preserve" language="JavaScript">
        hideblock('fs_abs');
</script>
<script xml:space="preserve" language="JavaScript">
        hideblock('surf_abs');
</script>
<script xml:space="preserve" language="JavaScript">
        hideblock('goat_abs');
</script>
<script xml:space="preserve" language="JavaScript">
        hideblock('sea_abs');
</script>
<script xml:space="preserve" language="JavaScript">
    hideblock('578_abs');
</script>
<script xml:space="preserve" language="JavaScript">
    hideblock('701_abs');
</script>
<script xml:space="preserve" language="JavaScript">
    hideblock('720_abs');
</script>
<script xml:space="preserve" language="JavaScript">
    hideblock('112_abs');
</script>
<script xml:space="preserve" language="JavaScript">
        hideblock('tmo_abs');
</script>
<script xml:space="preserve" language="JavaScript">
        hideblock('disc_abs');
</script>